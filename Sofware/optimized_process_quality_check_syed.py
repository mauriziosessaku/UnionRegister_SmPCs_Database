# -*- coding: utf-8 -*-
"""Optimized_process_quality_check_Syed

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PktPoT818RR2SURZqvTccRHjgks419qj
"""

!pip install PyMuPDF

!pip install fitz tools requests

!pip install "camelot-py[cv]"

!pip install PyPDF2

!pip install pdfplumber

"""START FROM HERE IF YOU ALREADY INSTALLED THE PACKAGES"""

"""
EMA Data Extraction - Colab Cell
With Date Filtering and link_final (NO MERGING)
Paste this entire code into a Colab cell and run it
"""

import os
import requests
import pandas as pd
import re
import json
from datetime import datetime

# ---------------------------
# Configuration
# ---------------------------
output_folder = "/content"
os.makedirs(output_folder, exist_ok=True)

# ---------------------------
# Web Scraping Functions
# ---------------------------

def extract_json_from_js(html_content):
    """Extract JSON data from JavaScript variables in the HTML"""
    pattern_product = r'var\s+dataSet_product_information\s*=\s*(\[.*?\]);'
    match_product = re.search(pattern_product, html_content, re.DOTALL)

    product_data = []
    if match_product:
        try:
            product_data = json.loads(match_product.group(1))
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Error parsing product JSON: {e}")

    pattern_proc = r'var\s+dataSet_proc\s*=\s*(\[.*?\]);'
    match_proc = re.search(pattern_proc, html_content, re.DOTALL)

    proc_data = []
    if match_proc:
        try:
            proc_data = json.loads(match_proc.group(1))
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Error parsing procedures JSON: {e}")

    return product_data, proc_data

def parse_product_information(product_data):
    """Parse the product information from dataSet_product_information"""
    metadata = {
        'name': '',
        'eu_num': '',
        'inn': '',
        'indication': '',
        'mah': '',
        'atc': '',
        'ema_link': ''
    }

    for item in product_data:
        item_type = item.get('type', '')
        value = item.get('value', '')

        if item_type == 'name':
            metadata['name'] = value
        elif item_type == 'eu_num':
            metadata['eu_num'] = value
        elif item_type == 'inn':
            metadata['inn'] = value
        elif item_type == 'indication':
            value = re.sub(r'<[^>]+>', '', value)
            metadata['indication'] = value
        elif item_type == 'mah':
            metadata['mah'] = value
        elif item_type == 'atc':
            meta = item.get('meta', [[]])
            if meta and len(meta) > 0 and len(meta[0]) > 0:
                for atc_item in meta[0]:
                    if atc_item.get('level') == '5':
                        metadata['atc'] = atc_item.get('code', '')
                        break
        elif item_type == 'ema_links':
            meta = item.get('meta', [])
            if meta and len(meta) > 0:
                metadata['ema_link'] = meta[0].get('url', '')

    return metadata

def parse_procedures(proc_data):
    """Parse the procedures data from dataSet_proc - NOW INCLUDES ID"""
    procedures = []

    for proc in proc_data:
        procedure = {
            'id': proc.get('id', ''),
            'closed': proc.get('closed', ''),
            'type': proc.get('type', ''),
            'ema_number': proc.get('ema_number', ''),
            'decision.number': proc.get('decision', {}).get('number', ''),
            'decision.date': proc.get('decision', {}).get('date', '')
        }
        procedures.append(procedure)

    return procedures

def build_link_final(id_value, decision_date):
    """
    Build the final PDF link from metadata
    Format: https://ec.europa.eu/health/documents/community-register/{year}/{date}{id}/anx_{id}_en.pdf

    Example: https://ec.europa.eu/health/documents/community-register/2014/20140305127843/anx_127843_en.pdf
    """
    try:
        if not decision_date or pd.isna(decision_date) or decision_date == '':
            return None

        if not id_value or pd.isna(id_value) or id_value == '':
            return None

        # Extract year from decision.date (format: YYYY-MM-DD)
        year = str(decision_date).split('-')[0]
        if len(year) != 4:
            return None

        # Format date as YYYYMMDD (remove dashes)
        date_formatted = str(decision_date).replace('-', '')

        # Build the URL
        base_url = "https://ec.europa.eu/health/documents/community-register"
        link = f"{base_url}/{year}/{date_formatted}{id_value}/anx_{id_value}_en.pdf"

        return link

    except Exception as e:
        return None

def scrape_ema_webpage(url):
    """Scrape metadata from EMA product webpage"""
    print(f"üåê Fetching webpage: {url}")

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()

    html_content = response.text

    print("üîç Extracting JSON data from webpage...")
    product_data, proc_data = extract_json_from_js(html_content)

    if not product_data:
        print("‚ö†Ô∏è No product data found in webpage")
        return None, None

    print(f"‚úÖ Found {len(product_data)} product information items")
    print(f"‚úÖ Found {len(proc_data)} procedure records")

    product_metadata = parse_product_information(product_data)
    product_metadata['webpage_url'] = url
    product_metadata['scraped_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    procedures = parse_procedures(proc_data)

    return product_metadata, procedures

def filter_by_dates(df_scraped, date_list):
    """Filter dataframe to only include rows with closed dates in the provided list"""
    print(f"\nüîç Filtering by {len(date_list)} date(s)...")

    # Filter the dataframe
    df_filtered = df_scraped[df_scraped['closed'].isin(date_list)]

    print(f"‚úÖ Filtered from {len(df_scraped)} to {len(df_filtered)} procedure records")

    if len(df_filtered) == 0:
        print("‚ö†Ô∏è No procedures found matching the provided dates!")
        print(f"   Dates you provided: {date_list}")
        print(f"   Available dates in data: {df_scraped['closed'].unique().tolist()}")

    return df_filtered

# ---------------------------
# Main Workflow
# ---------------------------

print("\n" + "="*80)
print("üöÄ EMA Data Extraction")
print("="*80 + "\n")

# Step 1: Get EMA webpage URL
print("STEP 1: EMA Webpage Scraping")
print("-" * 80)
ema_url = input("Enter EMA webpage URL: ").strip()

if not ema_url.startswith(('http://', 'https://')):
    print("‚ùå Invalid URL!")
    raise ValueError("Invalid URL provided")

# Scrape the webpage
try:
    product_metadata, procedures = scrape_ema_webpage(ema_url)

    if not product_metadata or not procedures:
        print("‚ùå Failed to extract data from webpage")
        raise ValueError("Failed to extract data")

    # Create DataFrame - INCLUDES ID
    df_scraped = pd.DataFrame({
        'id': [proc['id'] for proc in procedures],
        'closed': [proc['closed'] for proc in procedures],
        'type': [proc['type'] for proc in procedures],
        'ema_number': [proc['ema_number'] for proc in procedures],
        'decision.number': [proc['decision.number'] for proc in procedures],
        'decision.date': [proc['decision.date'] for proc in procedures],
        'ema_link': [product_metadata.get('ema_link', '') for _ in procedures],
        'name': [product_metadata.get('name', '') for _ in procedures],
        'eu_num': [product_metadata.get('eu_num', '') for _ in procedures],
        'inn': [product_metadata.get('inn', '') for _ in procedures],
        'indication': [product_metadata.get('indication', '') for _ in procedures],
        'mah': [product_metadata.get('mah', '') for _ in procedures],
        'atc': [product_metadata.get('atc', '') for _ in procedures]
    })

    # Build link_final column
    print("\nüîó Building PDF links...")
    df_scraped['link_final'] = df_scraped.apply(
        lambda row: build_link_final(row['id'], row['decision.date']),
        axis=1
    )

    valid_links = df_scraped['link_final'].notna().sum()
    print(f"‚úÖ Built {valid_links} out of {len(df_scraped)} PDF links")

    print(f"\n‚úÖ Scraped data: {len(df_scraped)} procedure records")

except Exception as e:
    print(f"‚ùå Error during scraping: {e}")
    import traceback
    traceback.print_exc()
    raise

# Step 2: Filter by dates (Optional)
print("\n" + "="*80)
print("STEP 2: Filter by Closed Dates (Optional)")
print("-" * 80)
print("Enter closed dates to filter (comma-separated), or press Enter to skip filtering")
print("Example: 2014-03-07, 2015-01-21, 2016-01-11")

dates_input = input("Dates to filter: ").strip()

if dates_input:
    # Parse the dates
    date_list = [date.strip() for date in dates_input.split(',')]
    print(f"üìã You entered {len(date_list)} date(s): {date_list}")

    # Filter the dataframe
    df_scraped = filter_by_dates(df_scraped, date_list)

    if len(df_scraped) == 0:
        print("‚ùå No matching procedures found. Exiting.")
        raise ValueError("No procedures matching the provided dates")
else:
    print("‚è≠Ô∏è  Skipping date filter - using all procedures")

# Step 3: Display and Save
print("\n" + "="*80)
print("STEP 3: Save Results")
print("-" * 80)

# Display preview
print("\nüìä Preview (first 10 rows):")
from IPython.display import display
display(df_scraped.head(10))

# Save to Excel
product_name = df_scraped['name'].iloc[0]
safe_name = re.sub(r'[^\w\-_]', '_', product_name.lower())
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

output_filename = f"{safe_name}_procedures_{timestamp}.xlsx"
output_path = os.path.join(output_folder, output_filename)

df_scraped.to_excel(output_path, index=False)

print(f"\nüíæ Saved to: {output_path}")

# Final summary
print("\n" + "="*80)
print("‚úÖ EXTRACTION COMPLETE!")
print("="*80)
print(f"Product: {product_name}")
print(f"Total Procedures: {len(df_scraped)}")
print(f"Valid PDF Links: {df_scraped['link_final'].notna().sum()}")
print(f"Output File: {output_path}")
print("="*80 + "\n")

# Store results in variables for further use
result_df = df_scraped
result_path = output_path

print("üí° The data is now available in the variable: df_scraped")
print("üí° You can now use df_scraped for further processing")

# @title
"""
Extract Section 4.8 from Multiple PDFs
Iterates through all link_final in df_scraped
Paste this code into a Colab cell and run it AFTER the previous cell
"""

import requests
import PyPDF2
import tempfile
import os
import re
import pandas as pd

# ---------------------------
# PDF download and extraction functions
# ---------------------------
def download_pdf(url, output_path):
    """Download PDF from URL"""
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    with open(output_path, "wb") as f:
        f.write(r.content)

def extract_text(pdf_path):
    """Extract text from PDF"""
    full_text = ""
    with open(pdf_path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            full_text += page.extract_text() or ""
    return full_text


def extract_section_4_8(text):
    """Extract section 4.8 with detailed debugging"""

    # First, let's see ALL occurrences of "4.8" in the text
    all_matches = list(re.finditer(r"4\.8", text, re.IGNORECASE))
    print(f"üîç Found {len(all_matches)} occurrences of '4.8' in the text")

    # Show context for each occurrence
    for i, match in enumerate(all_matches[:5], 1):  # Show first 5
        start = max(0, match.start() - 30)
        end = min(len(text), match.end() + 100)
        context = text[start:end]
        print(f"\n--- Occurrence {i} at position {match.start()} ---")
        print(repr(context))  # repr shows \n, \t, spaces explicitly
        print("---")

    # Now try to find "4.8" followed by "Undesirable" (with flexible spacing)
    print("\n\nüîç Looking for '4.8' + 'Undesirable'...")

    pattern = r"4\.8\s+Undesirable"
    matches = list(re.finditer(pattern, text, re.IGNORECASE))
    print(f"Found {len(matches)} matches")

    for i, match in enumerate(matches, 1):
        start = max(0, match.start() - 30)
        end = min(len(text), match.end() + 150)
        context = text[start:end]
        print(f"\n--- Match {i} at position {match.start()} ---")
        print(repr(context))
        print("---")

    if not matches:
        print("‚ùå No matches found!")
        return ""

    # Use the FIRST match
    first_match = matches[0]
    start_pos = first_match.end()

    print(f"\n‚úÖ Using FIRST match at position {first_match.start()}")

    # Find end
    end_match = re.search(r"4\.9", text[start_pos:], re.IGNORECASE)
    if end_match:
        end_pos = start_pos + end_match.start()
        print(f"‚úÖ Found '4.9' at position {end_pos}")
    else:
        end_pos = min(start_pos + 10000, len(text))
        print(f"‚ö†Ô∏è No '4.9' found, taking 10000 chars")

    section_text = text[start_pos:end_pos].strip()

    # Show first 500 chars of result
    print(f"\nüìÑ Extracted section preview (first 500 chars):")
    print("-" * 80)
    print(section_text[:500])
    print("-" * 80)

    # Remove duplicates
    seen = set()
    unique_lines = []
    for line in section_text.splitlines():
        line = line.strip()
        if line and line not in seen:
            seen.add(line)
            unique_lines.append(line)

    return "\n".join(unique_lines)

# ---------------------------
# Main iteration workflow
# ---------------------------

print("\n" + "="*80)
print("üìÑ Extract Section 4.8 from All PDFs in df_scraped")
print("="*80 + "\n")

# Check if df_scraped exists
try:
    df_scraped
    print(f"‚úÖ Found df_scraped with {len(df_scraped)} rows")
except NameError:
    print("‚ùå Error: df_scraped not found!")
    print("   Please run the EMA scraping cell first.")
    raise

# Get unique link_final
if 'link_final' not in df_scraped.columns:
    print("‚ùå Error: 'link_final' column not found in df_scraped")
    raise ValueError("Missing link_final column")

# Get unique links, excluding None/NaN values
unique_links = df_scraped['link_final'].dropna().unique()
print(f"üìä Found {len(unique_links)} unique PDF link(s)\n")

if len(unique_links) == 0:
    print("‚ö†Ô∏è No PDF links found in df_scraped")
    raise ValueError("No PDF links available")

# Create output folder
output_folder = "/content/section_4_8_extracts"
os.makedirs(output_folder, exist_ok=True)

# Track results
results = []
successful = 0
failed = 0

# Iterate through each unique link
for idx, pdf_url in enumerate(unique_links, 1):
    print(f"\n{'‚îÄ'*80}")
    print(f"Processing PDF {idx}/{len(unique_links)}")
    print(f"{'‚îÄ'*80}")

    if not pdf_url or pdf_url == '':
        print(f"‚ö†Ô∏è Skipping empty URL")
        failed += 1
        continue

    print(f"üîó URL: {pdf_url}")

    # Get product name and closed date for this link
    product_rows = df_scraped[df_scraped['link_final'] == pdf_url]
    if len(product_rows) > 0:
        product_name = product_rows.iloc[0]['name']
        closed_date = product_rows.iloc[0]['closed']
        procedure_type = product_rows.iloc[0]['type']

        # Create safe filename with date and name
        safe_name = re.sub(r'[^\w\-_]', '_', product_name.lower())
        safe_closed = str(closed_date).replace('/', '-').replace(' ', '_')
        filename_prefix = f"{safe_closed}_{safe_name}"
    else:
        product_name = 'Unknown'
        closed_date = 'Unknown'
        procedure_type = 'Unknown'
        filename_prefix = f"product_{idx}"

    print(f"üì¶ Product: {product_name}")
    print(f"üìÖ Closed Date: {closed_date}")
    print(f"üìã Type: {procedure_type}")

    with tempfile.TemporaryDirectory() as tmpdir:
        pdf_path = os.path.join(tmpdir, "temp.pdf")

        try:
            # Download PDF
            print(f"üì• Downloading PDF...")
            download_pdf(pdf_url, pdf_path)
            print(f"‚úÖ Download complete")

            # Extract text
            print(f"üìÑ Extracting text from PDF...")
            text = extract_text(pdf_path)
            print(f"‚úÖ Text extracted ({len(text)} characters)")

            # Extract section 4.8
            print(f"üîç Searching for section 4.8...")
            section_text = extract_section_4_8(text)

            if section_text:
                print(f"‚úÖ Section 4.8 found ({len(section_text)} characters)")

                # Save to file with date_name format
                txt_file = os.path.join(output_folder, f"{filename_prefix}_section_4_8.txt")
                with open(txt_file, "w", encoding="utf-8") as f:
                    f.write(section_text)

                print(f"üíæ Saved to: {txt_file}")

                results.append({
                    'product_name': product_name,
                    'closed_date': closed_date,
                    'type': procedure_type,
                    'pdf_url': pdf_url,
                    'output_file': txt_file,
                    'status': 'Success',
                    'section_length': len(section_text)
                })
                successful += 1

            else:
                print(f"‚ö†Ô∏è Section 4.8 not found in PDF")
                results.append({
                    'product_name': product_name,
                    'closed_date': closed_date,
                    'type': procedure_type,
                    'pdf_url': pdf_url,
                    'output_file': None,
                    'status': 'Section not found',
                    'section_length': 0
                })
                failed += 1

        except Exception as e:
            print(f"‚ùå Error processing PDF: {e}")
            results.append({
                'product_name': product_name,
                'closed_date': closed_date,
                'type': procedure_type,
                'pdf_url': pdf_url,
                'output_file': None,
                'status': f'Error: {str(e)}',
                'section_length': 0
            })
            failed += 1

# Create summary DataFrame
df_results = pd.DataFrame(results)

# Final summary
print("\n" + "="*80)
print("‚úÖ EXTRACTION COMPLETE!")
print("="*80)
print(f"Total PDFs processed: {len(unique_links)}")
print(f"‚úÖ Successful: {successful}")
print(f"‚ùå Failed: {failed}")
print(f"üìÅ Output folder: {output_folder}")
print("="*80 + "\n")

# Display results
print("üìä Extraction Results:")
from IPython.display import display
display(df_results)

# Save summary to Excel
summary_file = os.path.join(output_folder, "extraction_summary.xlsx")
df_results.to_excel(summary_file, index=False)
print(f"\nüíæ Summary saved to: {summary_file}")

# Store results for further use
extraction_results = df_results

""" NEW ONE
Extract Section 4.8 from Multiple PDFs (Robust Version)
Uses pdfplumber instead of PyPDF2 to fix 'Invalid Stream' errors.
"""

import requests
import pdfplumber  # Replaces PyPDF2
import tempfile
import os
import re
import pandas as pd

# ---------------------------
# PDF download and extraction functions
# ---------------------------
def download_pdf(url, output_path):
    """Download PDF from URL"""
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        with open(output_path, "wb") as f:
            f.write(r.content)
    except Exception as e:
        print(f"Error downloading {url}: {e}")
        raise

def extract_text(pdf_path):
    """Extract text using pdfplumber (more robust than PyPDF2)"""
    full_text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                # extract_text() can return None if page is empty
                text = page.extract_text()
                if text:
                    full_text += text + "\n"
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: pdfplumber encountered an issue: {e}")
    return full_text

def extract_section_4_8(text):
    """Extract section 4.8 with detailed debugging"""

    # Regex: "4.8" + optional punctuation + whitespace/newlines + "Un"
    # This handles "4.8 Undesirable", "4.8\nUndesirable", "4.8. Undesirable"
    pattern = r"4\.8[:\.]?\s+Un"

    print(f"\nüîç Looking for pattern: {pattern} (Case Insensitive)")

    matches = list(re.finditer(pattern, text, re.IGNORECASE))
    print(f"Found {len(matches)} matches")

    if not matches:
        print("‚ùå No matches found!")
        # Debug helper: print first few occurrences of just "4.8" to see what's wrong
        simple_matches = list(re.finditer(r"4\.8", text))
        if simple_matches:
            print("   However, simple '4.8' was found. Check context:")
            m = simple_matches[0]
            print(f"   Context: {repr(text[m.start():m.end()+20])}")
        return ""

    # Use the FIRST match
    first_match = matches[0]
    start_pos = first_match.start() # Start at '4.8', not after 'Un'

    print(f"\n‚úÖ Using FIRST match at position {start_pos}")

    # Find end (Section 4.9)
    # Looking for 4.9 followed by optional dot/colon and whitespace or digit
    end_match = re.search(r"4\.9[:\.]?\s", text[start_pos:], re.IGNORECASE)

    if end_match:
        end_pos = start_pos + end_match.start()
        print(f"‚úÖ Found '4.9' at position {end_pos}")
    else:
        end_pos = min(start_pos + 10000, len(text))
        print(f"‚ö†Ô∏è No '4.9' found, taking 10000 chars limit")

    section_text = text[start_pos:end_pos].strip()

    # Deduplicate lines
    seen = set()
    unique_lines = []
    for line in section_text.splitlines():
        line = line.strip()
        if line and line not in seen:
            seen.add(line)
            unique_lines.append(line)

    return "\n".join(unique_lines)

# ---------------------------
# Main iteration workflow
# ---------------------------

print("\n" + "="*80)
print("üìÑ Extract Section 4.8 from All PDFs in df_scraped")
print("="*80 + "\n")

# Check if df_scraped exists
try:
    df_scraped
    print(f"‚úÖ Found df_scraped with {len(df_scraped)} rows")
except NameError:
    print("‚ùå Error: df_scraped not found!")
    raise

if 'link_final' not in df_scraped.columns:
    raise ValueError("Missing link_final column")

unique_links = df_scraped['link_final'].dropna().unique()

# Create output folder
output_folder = "/content/section_4_8_extracts"
os.makedirs(output_folder, exist_ok=True)

results = []
successful = 0
failed = 0

for idx, pdf_url in enumerate(unique_links, 1):
    print(f"\n{'‚îÄ'*80}")
    print(f"Processing PDF {idx}/{len(unique_links)}")

    if not pdf_url or pdf_url == '':
        print(f"‚ö†Ô∏è Skipping empty URL")
        failed += 1
        continue

    # Get product metadata
    product_rows = df_scraped[df_scraped['link_final'] == pdf_url]
    if len(product_rows) > 0:
        product_name = product_rows.iloc[0]['name']
        closed_date = product_rows.iloc[0]['closed']
        safe_name = re.sub(r'[^\w\-_]', '_', str(product_name).lower())
        safe_closed = str(closed_date).replace('/', '-').replace(' ', '_')
        filename_prefix = f"{safe_closed}_{safe_name}"
    else:
        product_name = 'Unknown'
        filename_prefix = f"product_{idx}"

    print(f"üì¶ Product: {product_name}")

    with tempfile.TemporaryDirectory() as tmpdir:
        pdf_path = os.path.join(tmpdir, "temp.pdf")
        try:
            download_pdf(pdf_url, pdf_path)

            # Using the new robust extraction
            text = extract_text(pdf_path)

            if len(text) < 100:
                print("‚ö†Ô∏è Warning: Extracted text is very short/empty. PDF might be image-based.")

            section_text = extract_section_4_8(text)

            if section_text:
                txt_file = os.path.join(output_folder, f"{filename_prefix}_section_4_8.txt")
                with open(txt_file, "w", encoding="utf-8") as f:
                    f.write(section_text)

                print(f"üíæ Saved to: {txt_file}")
                results.append({
                    'product_name': product_name,
                    'pdf_url': pdf_url,
                    'status': 'Success',
                    'length': len(section_text)
                })
                successful += 1
            else:
                print(f"‚ö†Ô∏è Section 4.8 not found")
                results.append({
                    'product_name': product_name,
                    'pdf_url': pdf_url,
                    'status': 'Section 4.8 Not Found',
                    'length': 0
                })
                failed += 1

        except Exception as e:
            print(f"‚ùå Error: {e}")
            results.append({
                'product_name': product_name,
                'pdf_url': pdf_url,
                'status': f'Error: {e}',
                'length': 0
            })
            failed += 1

df_results = pd.DataFrame(results)
print(f"\n‚úÖ Done. Success: {successful}, Failed: {failed}")
extraction_results = df_results

"""
Extract Adverse Events from All Section 4.8 Files
Uses DeepSeek API to extract adverse events from each txt file
Paste this code into a Colab cell and run it AFTER extracting section 4.8 files
"""

import os
import csv
import requests
import pandas as pd
import re
from glob import glob

# ---------------------------
# DeepSeek API call
# ---------------------------
def call_deepseek(prompt):
    """Call DeepSeek API to extract adverse events"""
    url = "https://api.deepseek.com/chat/completions"
    headers = {
        "Authorization": f"Bearer ....",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "deepseek-chat",
        "temperature": 0,
        "messages": [
            {"role": "system", "content": "You are an expert assistant that extracts structured data."},
            {"role": "user", "content": prompt}
        ]
    }

    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    return response.json()["choices"][0]["message"]["content"].strip()

# ---------------------------
# Extract AEs from TXT using DeepSeek
# ---------------------------
def extract_adverse_events(txt_file):
    """Extract adverse events from section 4.8 text file"""
    with open(txt_file, "r", encoding="utf-8") as f:
        section_text = f.read()

    prompt = f"""
    You are given the undesirable effects section of a drug product.
    Extract ONLY the list of adverse events mentioned in the text.
    Each adverse event must be listed separately, without duplicates.
    Do not include any explanatory text, frequencies, or formatting, only the adverse events.
    Output them as a plain list, one per line.

    Text:
    {section_text}
    """

    ae_text = call_deepseek(prompt)
    adverse_events = [line.strip() for line in ae_text.split("\n") if line.strip()]
    return adverse_events

# ---------------------------
# Save AEs to CSV
# ---------------------------
def save_to_csv(adverse_events, output_file, product_name):
    """Save adverse events to CSV file"""
    with open(output_file, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([product_name])  # column header
        for ae in adverse_events:
            writer.writerow([ae])
    print(f"‚úÖ CSV saved: {output_file}")

# ---------------------------
# Main batch workflow
# ---------------------------

print("\n" + "="*80)
print("üî¨ Extract Adverse Events from All Section 4.8 Files")
print("="*80 + "\n")

# Define input and output folders
input_folder = "/content/section_4_8_extracts"
output_folder = "/content/adverse_events_extracts"
os.makedirs(output_folder, exist_ok=True)

# Find all section 4.8 txt files
txt_files = glob(os.path.join(input_folder, "*_section_4_8.txt"))

if len(txt_files) == 0:
    print(f"‚ùå No section 4.8 files found in {input_folder}")
    print("   Please run the section 4.8 extraction script first.")
    raise ValueError("No input files found")

print(f"üìä Found {len(txt_files)} section 4.8 file(s) to process\n")

# Track results
results = []
successful = 0
failed = 0

# Process each file
for idx, txt_file in enumerate(txt_files, 1):
    print(f"\n{'‚îÄ'*80}")
    print(f"Processing file {idx}/{len(txt_files)}")
    print(f"{'‚îÄ'*80}")

    # Extract filename info
    filename = os.path.basename(txt_file)
    print(f"üìÑ File: {filename}")

    # Parse filename to get date and product name
    # Expected format: YYYY-MM-DD_productname_section_4_8.txt
    match = re.match(r'(\d{4}-\d{2}-\d{2})_(.+?)_section_4_8\.txt', filename)

    if match:
        closed_date = match.group(1)
        product_name = match.group(2)
    else:
        # Fallback if filename doesn't match expected format
        closed_date = "unknown_date"
        product_name = filename.replace("_section_4_8.txt", "")

    print(f"üìÖ Date: {closed_date}")
    print(f"üì¶ Product: {product_name}")

    try:
        # Extract adverse events using DeepSeek
        print(f"ü§ñ Calling DeepSeek API to extract adverse events...")
        adverse_events = extract_adverse_events(txt_file)
        print(f"‚úÖ Extracted {len(adverse_events)} adverse events")

        # Create output filename with same format
        output_filename = f"{closed_date}_{product_name}_adverse_events.csv"
        output_file = os.path.join(output_folder, output_filename)

        # Save to CSV
        save_to_csv(adverse_events, output_file, product_name)

        # Track success
        results.append({
            'input_file': filename,
            'closed_date': closed_date,
            'product_name': product_name,
            'output_file': output_filename,
            'adverse_events_count': len(adverse_events),
            'status': 'Success'
        })
        successful += 1

    except Exception as e:
        print(f"‚ùå Error processing file: {e}")
        results.append({
            'input_file': filename,
            'closed_date': closed_date,
            'product_name': product_name,
            'output_file': None,
            'adverse_events_count': 0,
            'status': f'Error: {str(e)}'
        })
        failed += 1

# Create summary DataFrame
df_results = pd.DataFrame(results)

# Final summary
print("\n" + "="*80)
print("‚úÖ ADVERSE EVENTS EXTRACTION COMPLETE!")
print("="*80)
print(f"Total files processed: {len(txt_files)}")
print(f"‚úÖ Successful: {successful}")
print(f"‚ùå Failed: {failed}")
print(f"üìÅ Output folder: {output_folder}")
print("="*80 + "\n")

# Display results
print("üìä Extraction Results:")
from IPython.display import display
display(df_results)

# Save summary to Excel
summary_file = os.path.join(output_folder, "adverse_events_extraction_summary.xlsx")
df_results.to_excel(summary_file, index=False)
print(f"\nüíæ Summary saved to: {summary_file}")

# Store results for further use
ae_extraction_results = df_results

"""
Convert Adverse Events CSV to Excel with Metadata
Merges adverse events from CSV with product metadata from df_scraped
Paste this code into a Colab cell and run it AFTER extracting adverse events
"""

import os
import pandas as pd
import re
from glob import glob
from datetime import datetime

print("\n" + "="*80)
print("üìä Convert Adverse Events CSV to Excel with Metadata")
print("="*80 + "\n")

# Check if df_scraped exists
try:
    df_scraped
    print(f"‚úÖ Found df_scraped with {len(df_scraped)} rows")
except NameError:
    print("‚ùå Error: df_scraped not found!")
    print("   Please run the EMA scraping cell first.")
    raise

# Define input and output folders
input_folder = "/content/adverse_events_extracts"
output_folder = "/content/final_adverse_events"
os.makedirs(output_folder, exist_ok=True)

# Find all adverse events CSV files
csv_files = glob(os.path.join(input_folder, "*_adverse_events.csv"))

if len(csv_files) == 0:
    print(f"‚ùå No adverse events CSV files found in {input_folder}")
    print("   Please run the adverse events extraction script first.")
    raise ValueError("No input files found")

print(f"üìä Found {len(csv_files)} adverse events CSV file(s) to process\n")

# Track results
results = []
successful = 0
failed = 0

# Process each CSV file
for idx, csv_file in enumerate(csv_files, 1):
    print(f"\n{'‚îÄ'*80}")
    print(f"Processing file {idx}/{len(csv_files)}")
    print(f"{'‚îÄ'*80}")

    # Extract filename info
    filename = os.path.basename(csv_file)
    print(f"üìÑ File: {filename}")

    # Parse filename to get date and product name
    # Expected format: YYYY-MM-DD_productname_adverse_events.csv
    match = re.match(r'(\d{4}-\d{2}-\d{2})_(.+?)_adverse_events\.csv', filename)

    if match:
        closed_date = match.group(1)
        product_name = match.group(2)
    else:
        print(f"‚ö†Ô∏è Filename doesn't match expected format, skipping")
        failed += 1
        continue

    print(f"üìÖ Date: {closed_date}")
    print(f"üì¶ Product: {product_name}")

    try:
        # Read the CSV file (first column is header, rest are adverse events)
        df_csv = pd.read_csv(csv_file, header=0)

        # Get adverse events from first column
        adverse_events = df_csv.iloc[:, 0].dropna().tolist()
        print(f"‚úÖ Loaded {len(adverse_events)} adverse events from CSV")

        # Find matching row in df_scraped
        matching_rows = df_scraped[df_scraped['closed'] == closed_date]

        if len(matching_rows) == 0:
            print(f"‚ö†Ô∏è No matching metadata found in df_scraped for date {closed_date}")
            # Try with product name as fallback
            product_name_clean = product_name.replace('_', ' ')
            matching_rows = df_scraped[df_scraped['name'].str.lower().str.replace(' ', '_') == product_name]

            if len(matching_rows) == 0:
                print(f"‚ö†Ô∏è Still no match found, skipping")
                failed += 1
                continue

        # Get metadata from first matching row
        metadata_row = matching_rows.iloc[0]

        print(f"‚úÖ Found metadata for: {metadata_row['name']}")

        # Create DataFrame with metadata repeated for each adverse event
        df_output = pd.DataFrame({
            'closed': [metadata_row['closed'] for _ in adverse_events],
            'type': [metadata_row['type'] for _ in adverse_events],
            'ema_number': [metadata_row['ema_number'] for _ in adverse_events],
            'decision.number': [metadata_row['decision.number'] for _ in adverse_events],
            'decision.date': [metadata_row['decision.date'] for _ in adverse_events],
            'ema_link': [metadata_row['link_final'] for _ in adverse_events],  # ‚Üê USE link_final
            'name': [metadata_row['name'] for _ in adverse_events],
            'eu_num': [metadata_row['eu_num'] for _ in adverse_events],
            'inn': [metadata_row['inn'] for _ in adverse_events],
            'indication': [metadata_row['indication'] for _ in adverse_events],
            'mah': [metadata_row['mah'] for _ in adverse_events],
            'atc': [metadata_row['atc'] for _ in adverse_events],
            'adverse_event': adverse_events
        })

        # Create output filename
        output_filename = f"{closed_date}_{product_name}_adverse_events.xlsx"
        output_file = os.path.join(output_folder, output_filename)

        # Save to Excel
        df_output.to_excel(output_file, index=False)
        print(f"üíæ Saved: {output_file}")

        # Track success
        results.append({
            'input_file': filename,
            'closed_date': closed_date,
            'product_name': metadata_row['name'],
            'output_file': output_filename,
            'adverse_events_count': len(adverse_events),
            'status': 'Success'
        })
        successful += 1

    except Exception as e:
        print(f"‚ùå Error processing file: {e}")
        import traceback
        traceback.print_exc()
        results.append({
            'input_file': filename,
            'closed_date': closed_date,
            'product_name': product_name,
            'output_file': None,
            'adverse_events_count': 0,
            'status': f'Error: {str(e)}'
        })
        failed += 1

# Create summary DataFrame
df_results = pd.DataFrame(results)

# Final summary
print("\n" + "="*80)
print("‚úÖ CONVERSION COMPLETE!")
print("="*80)
print(f"Total files processed: {len(csv_files)}")
print(f"‚úÖ Successful: {successful}")
print(f"‚ùå Failed: {failed}")
print(f"üìÅ Output folder: {output_folder}")
print("="*80 + "\n")

# Display results
print("üìä Conversion Results:")
from IPython.display import display
display(df_results)

# Save summary to Excel
summary_file = os.path.join(output_folder, "conversion_summary.xlsx")
df_results.to_excel(summary_file, index=False)
print(f"\nüíæ Summary saved to: {summary_file}")

# Store results for further use
conversion_results = df_results

"""
Download All Output Folders
Creates ZIP files for section 4.8 txt files, adverse events CSV, and final Excel files
Paste this code into a Colab cell and run it to download everything
"""

import os
import shutil
from datetime import datetime
from google.colab import files

print("\n" + "="*80)
print("üì¶ Preparing Downloads")
print("="*80 + "\n")

# Define folders to zip
folders_to_download = [
    {
        'path': '/content/section_4_8_extracts',
        'name': 'section_4_8_txt_files',
        'description': 'Section 4.8 Text Files'
    },
    {
        'path': '/content/adverse_events_extracts',
        'name': 'adverse_events_csv_files',
        'description': 'Adverse Events CSV Files'
    },
    {
        'path': '/content/final_adverse_events',
        'name': 'final_adverse_events_xlsx',
        'description': 'Final Adverse Events Excel Files'
    }
]

# Create a timestamp for the download
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# Track download info
downloads = []

print("üóúÔ∏è  Creating ZIP files...\n")

for folder_info in folders_to_download:
    folder_path = folder_info['path']
    zip_name = f"{folder_info['name']}_{timestamp}"
    description = folder_info['description']

    print(f"{'‚îÄ'*80}")
    print(f"üìÅ {description}")
    print(f"{'‚îÄ'*80}")

    # Check if folder exists
    if not os.path.exists(folder_path):
        print(f"‚ö†Ô∏è  Folder not found: {folder_path}")
        print(f"   Skipping...\n")
        continue

    # Count files in folder
    files_in_folder = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
    file_count = len(files_in_folder)

    if file_count == 0:
        print(f"‚ö†Ô∏è  Folder is empty: {folder_path}")
        print(f"   Skipping...\n")
        continue

    print(f"üìä Found {file_count} file(s)")

    # Create ZIP file
    zip_file = f"/content/{zip_name}"
    print(f"üóúÔ∏è  Creating ZIP: {zip_name}.zip")

    try:
        shutil.make_archive(zip_file, 'zip', folder_path)
        zip_path = f"{zip_file}.zip"

        # Get file size
        file_size = os.path.getsize(zip_path)
        size_mb = file_size / (1024 * 1024)

        print(f"‚úÖ ZIP created: {size_mb:.2f} MB")

        downloads.append({
            'description': description,
            'zip_file': zip_path,
            'file_count': file_count,
            'size_mb': size_mb
        })

    except Exception as e:
        print(f"‚ùå Error creating ZIP: {e}")

    print()

# Summary
print("="*80)
print("üì¶ Download Summary")
print("="*80)

if len(downloads) == 0:
    print("‚ö†Ô∏è  No files to download!")
    print("   Make sure you've run the extraction scripts first.")
else:
    total_size = sum([d['size_mb'] for d in downloads])
    total_files = sum([d['file_count'] for d in downloads])

    print(f"Total ZIP files: {len(downloads)}")
    print(f"Total files: {total_files}")
    print(f"Total size: {total_size:.2f} MB\n")

    for i, download in enumerate(downloads, 1):
        print(f"{i}. {download['description']}")
        print(f"   Files: {download['file_count']} | Size: {download['size_mb']:.2f} MB")

    print("\n" + "="*80)
    print("‚¨áÔ∏è  Starting Downloads...")
    print("="*80 + "\n")

    # Download each ZIP file
    for download in downloads:
        print(f"‚¨áÔ∏è  Downloading: {os.path.basename(download['zip_file'])}")
        try:
            files.download(download['zip_file'])
            print(f"‚úÖ Download started\n")
        except Exception as e:
            print(f"‚ùå Download failed: {e}\n")

    print("="*80)
    print("‚úÖ ALL DOWNLOADS COMPLETE!")
    print("="*80)
    print("\nüí° TIP: Check your browser's download folder for the ZIP files")
    print("üí° Extract the ZIP files to access your data\n")

import os
import shutil

content_dir = "/content"

for item in os.listdir(content_dir):
    item_path = os.path.join(content_dir, item)
    if os.path.isfile(item_path) or os.path.islink(item_path):
        os.unlink(item_path)
    elif os.path.isdir(item_path):
        shutil.rmtree(item_path)

from google.colab import drive
drive.mount('/content/drive')

"""END"""